{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Démarrage avec JupyterHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1ère Commande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = spark.sql(\"select * from default.basedos\")\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution d'une requète SQL simple avec Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "requete_SQL = \"select count(*) from default.basedos where date_chargement=201801\"\n",
    "count_lignes=spark.sql(requete_SQL)\n",
    "count_lignes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Creation d'un dataframe spark a partir d'une table depuis DataSmart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ma_table=spark.table(\"default.instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comptage lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_lignes=df_ma_table.count()\n",
    "print(nb_lignes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Affichage structure table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colonnes_types=df_ma_table.dtypes\n",
    "for i in range(len(colonnes_types)):\n",
    "    print(\"%s : %s\"%(colonnes_types[i][0],colonnes_types[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Affichage 10 lignes , sur les 10 ere variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ma_table.select(df_ma_table.columns[0:10]).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Affichage des partitions disponibles et de leur volumétrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df_ma_table.groupBy(\"date_chargement\").agg(count(\"date_chargement\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Manipulation d'une table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ma_table_filtree=df_ma_table.filter(\"date_chargement=20180515 and nomenseigne ='CETELEM'\")\n",
    "df_ma_table_filtree.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selection colonnes et renommage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_ma_table_2=(df_ma_table_filtree.select(col(\"NUMAUTO\").alias(\"Contrat\"),\n",
    "                                         col(\"CODPRODUITALPHASICLID\").alias(\"Produit\"),\n",
    "                                         col(\"TITDATENAISSANCE\").alias(\"DateNaissance\"))\n",
    "                                 .orderBy('Contrat'))\n",
    "df_ma_table_2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Suppression doublon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ma_table_3=df_ma_table_2.distinct().orderBy('Contrat')\n",
    "df_ma_table_3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ajout d'un champs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.types import IntegerType\n",
    "df_ma_table_4=df_ma_table_3.withColumn(\"AnneeNaissance\", substring(\"DateNaissance\",0,4).cast(IntegerType()))\n",
    "df_ma_table_4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Suppression de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ma_table_5=df_ma_table_4.drop(\"DateNaissance\")\n",
    "df_ma_table_5.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistiques de base sur les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ma_table_5.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Export csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Export dataframe Spark vers fichier HDFS - distribué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number,col\n",
    "from pyspark.sql.window import Window\n",
    "extrait_df_ma_table_5=(df_ma_table_5.withColumn(\"rank\", row_number().over(Window.orderBy(col(\"AnneeNaissance\"))))\n",
    "                                .filter(\"rank < 100\"))\n",
    "                                \n",
    "extrait_df_ma_table_5.persist().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "destination=\"hdfs:///data/TP/EXPORT/lc78355/\"\n",
    "nom_export=\"extrait_df\"\n",
    "try:\n",
    "    extrait_df_ma_table_5.coalesce(1).write.option(\"header\", \"true\").option(\"delimiter\", \";\").parquet(destination+nom_export)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "extrait_df_ma_table_5.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
